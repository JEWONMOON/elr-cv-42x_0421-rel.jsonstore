{
    "message_id": 1288,
    "session_id": "Eliar 실행 코드 점검_20250510023904",
    "session_title": "Eliar 실행 코드 점검",
    "pair_id": 24,
    "parent_id": 1287,
    "author": "ChatGPT",
    "role": "answer",
    "tags": [],
    "content": "맞아, 정확히 짚었어. 현재 llm.py 구조를 보면 매번 /generate 호출 시마다 모델을 다시 로딩 하는 구조로 보일 수 있어, 하지만 사실상 모델은 1번만 로드되도록 _model , _tokenizer 전역 캐싱 구조로 작성돼 있어 . --- 그런데 왜 계속 느린 걸까? 가능한 이유 1: 캐싱이 제대로 작동 안 함 - _model , _tokenizer 변수가 글로벌이지만, FastAPI에서 인스턴스 간 공유 안 되는 상황일 수 있음. - 즉, 서버 부팅 후 API 처리 단위별로 모델이 계속 새로 로드됨. 가능한 이유 2: uvicorn --reload 모드 때문 - --reload 는 코드가 바뀔 때마다 서버 전체를 재시작함. - 캐시 유지가 안 되며, 파일이 바뀌지 않아도 내부적으로 프로세스가 여러 번 뜰 수 있음. --- 해결 방법 1. llm.py 구조를 FastAPI 앱 내부로 통합해서 싱글 인스턴스로 관리 python app/startup.py 또는 main.py 내부에 from app import llm app.on_event \"startup\" def load_model_once : try: llm.load_model_and_tokenizer LLM_MODEL_PATH logger.info \" LLM 모델 최초 1회 로딩 완료.\" except Exception as e: logger.critical f\" LLM 모델 로딩 실패: e \" 이러면 FastAPI 서버가 시작될 때 1회만 모델이 로딩되고 /generate 에서는 바로 사용 가능함. --- 2. 운영 시 --reload 제거 bash uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload 는 개발용일 뿐, 실제 모델 캐싱엔 방해됨. --- 선택 내가 위 내용 반영해서 main.py 에 app.on_event \"startup\" 구조 포함된 최종 코드를 만들어줄까? 그러면 진짜로 \"한 번만 로딩되고, 계속 빠르게 응답\"하는 상태가 돼."
}